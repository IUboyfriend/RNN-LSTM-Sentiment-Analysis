{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOQgNMRJVRas9+03+BaQNnN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **RNN**\n","\n","---\n","\n"],"metadata":{"id":"m5NlQGqqcztw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fs5eucG2DpQj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698916593121,"user_tz":-480,"elapsed":16639,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"9defa759-35ef-4a81-b36e-fcc46f68680c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G-HO1yf1Wikd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698916593121,"user_tz":-480,"elapsed":4,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"bf8adc99-792b-43d4-995a-a3a78172d745"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Learning/EIE4122/Lab2\n"]}],"source":["# Create a working folder and cd to it.\n","!mkdir -p /content/drive/MyDrive/Learning/EIE4122/Lab2\n","%cd /content/drive/MyDrive/Learning/EIE4122/Lab2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Atdg0a2hmG5z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698916600449,"user_tz":-480,"elapsed":7330,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"03fac3a0-1ce1-4237-8035-d217a0ebdee7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.4)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"]}],"source":["# Require gdown to download files from Google Drive\n","!pip install gdown"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zGwk4cI3LYKo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698916701146,"user_tz":-480,"elapsed":100704,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"0caf6895-6667-4948-c6c7-c2ce8c3c4ab2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Start decompression. It will take few minutes\n"]}],"source":["!echo \"Start decompression. It will take few minutes\"\n","!tar zxf data.tgz --exclude=\"._*\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LPMpWD9ia0di","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698916701584,"user_tz":-480,"elapsed":450,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"2c0d8364-1a41-4c95-92c2-856e56e1dbbe"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.0M\t.data/imdb/aclImdb/test/neg\n","2.2M\t.data/imdb/aclImdb/test/pos\n","25M\t.data/imdb/aclImdb/test\n","2.3M\t.data/imdb/aclImdb/train/pos\n","2.1M\t.data/imdb/aclImdb/train/neg\n","68M\t.data/imdb/aclImdb/train\n","94M\t.data/imdb/aclImdb\n"]}],"source":["# Make sure that decompression is successful. The command \"du\" lists the size of folders.\n","!du -h .data/imdb/aclImdb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jcOVs1TC7ZNx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698916905202,"user_tz":-480,"elapsed":203620,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"b491b8d8-561c-4c5c-b755-cbdf5d097eef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch==2.0.1\n","  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchtext==0.5.0\n","  Downloading torchtext-0.5.0-py3-none-any.whl (73 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.2)\n","Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n","  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n","  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n","  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n","  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n","  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n","  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n","  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting triton==2.0.0 (from torch==2.0.1)\n","  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.5.0) (4.66.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.5.0) (2.31.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.5.0) (1.23.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.5.0) (1.16.0)\n","Collecting sentencepiece (from torchtext==0.5.0)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (67.7.2)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.41.2)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.27.7)\n","Collecting lit (from triton==2.0.0->torch==2.0.1)\n","  Downloading lit-17.0.4.tar.gz (153 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.1/153.1 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.5.0) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.5.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.5.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.5.0) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n","Building wheels for collected packages: lit\n","  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for lit: filename=lit-17.0.4-py3-none-any.whl size=93257 sha256=dbc7e69954f06256a5caad89511d5b65b5b9b5f4a94e3ad318ceece2ccc99243\n","  Stored in directory: /root/.cache/pip/wheels/be/ae/00/696c57d438bfc7c0e89c4c379083ea08b1c2e54d85a5f7cd7c\n","Successfully built lit\n","Installing collected packages: sentencepiece, lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchtext\n","  Attempting uninstall: triton\n","    Found existing installation: triton 2.1.0\n","    Uninstalling triton-2.1.0:\n","      Successfully uninstalled triton-2.1.0\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.1.0+cu118\n","    Uninstalling torch-2.1.0+cu118:\n","      Successfully uninstalled torch-2.1.0+cu118\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.16.0\n","    Uninstalling torchtext-0.16.0:\n","      Successfully uninstalled torchtext-0.16.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n","torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n","torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed lit-17.0.4 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 sentencepiece-0.1.99 torch-2.0.1 torchtext-0.5.0 triton-2.0.0\n"]}],"source":["# Install torch 2.0.1 and torchtext 0.5.0\n","!pip install torch==2.0.1 torchtext==0.5.0\n","# you can safely ignore the message \"ERROR: pip's dependency resolver...\" for this lab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nZjfGnBGhN3I"},"outputs":[],"source":["# Define input TEXT and output LABEL\n","import torch\n","from torchtext import data\n","\n","SEED = 1234\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","TEXT = data.Field(tokenize = 'spacy',tokenizer_language = 'en_core_web_sm')\n","LABEL = data.LabelField(dtype = torch.float)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9K9X7Y2NhN3J"},"outputs":[],"source":["# Because data has been uploaded to \".data/imdb/aclImdb/\", this step will only take 1 minute.\n","# This step should be quick as PyTorch only needs to know the structure of the data in the\n","# folder \".data/imdb/aclImdb\". It will not actually load the data, which will be done by\n","# the Dataloader object (see code below)\n","from torchtext import datasets\n","train_data, test_data = datasets.IMDB.splits(TEXT, LABEL, path='.data/imdb/aclImdb/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iKo1CvNFhN3K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698916956632,"user_tz":-480,"elapsed":14,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"ce5017da-9cf4-44b0-9c84-f3d5c01b53e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training examples: 2780\n","Number of testing examples: 2780\n"]}],"source":["print(f'Number of training examples: {len(train_data)}')\n","print(f'Number of testing examples: {len(test_data)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j-7M0IehhN3L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698916956632,"user_tz":-480,"elapsed":5,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"9b6112ce-b5b3-4d43-e5cf-b26635e337b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'text': ['\"', 'Cinema', 'is', 'the', 'ultimate', 'pervert', 'art', '.', 'It', 'does', \"n't\", 'give', 'you', 'what', 'you', 'desire', ';', 'it', 'tells', 'you', 'how', 'to', 'desire', '.', '\"<br', '/><br', '/>So', 'begins', '\"', 'The', 'Pervert', \"'s\", 'Guide', 'to', 'Cinema', ',', '\"', 'in', 'which', 'Slovenian', 'philosopher', 'and', 'psychoanalyst', 'Slavoj', 'Zizek', 'applies', 'his', 'Freudian', '/', 'Lacanian', 'brain', '-', 'scalpel', 'to', 'world', 'cinema', '.', 'This', 'film', 'in', 'three', 'parts', 'is', 'the', 'second', 'feature', 'documentary', 'directed', 'by', 'Sophie', 'Fiennes', '(', 'yes', ',', 'sister', 'of', 'Ralph', 'and', 'Joseph', ')', ',', 'and', 'it', 'is', 'a', 'notable', 'accomplishment', ',', 'clocking', 'in', 'at', '2', '1/2', 'hours', 'of', 'talk', 'from', 'one', 'man', 'and', 'yet', 'remaining', 'humorous', 'and', 'engaging', 'throughout', '.', 'In', 'essence', ',', 'it', 'is', 'an', 'extended', 'film', 'lecture', ',', 'and', 'one', 'of', 'the', 'best', 'you', 'may', 'ever', 'get', '.', 'Over', 'the', 'course', 'of', 'the', 'film', ',', 'Zizek', 'guides', 'us', 'through', 'a', 'catalog', 'of', 'obsession', 'and', 'desire', 'in', 'film', 'history', '.', 'He', 'touches', 'on', 'more', 'than', '40', 'films', 'and', ',', 'in', 'particular', ',', 'spends', 'a', 'great', 'deal', 'of', 'time', 'with', 'Hitchcock', ',', 'Lynch', ',', 'Chaplin', ',', 'Tarkovsky', ',', 'the', 'Marx', 'Brothers', ',', 'and', 'Eisenstein', '.', 'But', 'he', 'also', 'takes', 'a', 'close', 'look', 'at', '\"', 'Persona', ',', '\"', '\"', 'The', 'Conversation', ',', '\"', '\"', 'Three', 'Colors', ':', 'Blue', ',', '\"', '\"', 'Dogville', ',', '\"', '\"', 'Fight', 'Club', ',', '\"', 'and', '\"', 'The', 'Exorcist', '.', '\"', 'Thematically', ',', 'Zizek', \"'s\", 'inquiry', 'into', 'cinema', 'ranges', 'from', 'thoughts', 'on', 'the', 'death', 'drive', 'to', 'the', '\"', 'coordinates', 'of', 'desire', ',', '\"', 'and', 'from', 'Gnosticism', 'to', '\"', 'partial', 'objects', '.', '\"<br', '/><br', '/>\"The', 'Pervert', \"'s\", 'Guide', '\"', 'will', 'be', 'a', 'slightly', 'better', 'experience', 'if', 'you', \"'ve\", 'taken', 'a', 'few', 'minutes', 'to', 'bone', 'up', 'on', 'your', 'basic', 'Freudian', 'terminology', '.', 'However', ',', 'even', 'if', 'you', \"'re\", 'not', 'steeped', 'in', 'psychoanalytic', 'theory', ',', 'Zizek', \"'s\", 'dynamic', 'and', 'hilarious', 'personality', 'carries', 'the', 'film', 'forward', 'with', 'such', 'gusto', 'that', 'you', 'are', \"n't\", 'likely', 'to', 'balk', 'at', 'the', 'specialized', 'lingo', '.', 'The', 'film', 'frequently', 'cuts', 'from', 'movie', 'clips', 'to', 'images', 'of', 'Zizek', '*', 'inside', '*', 'the', 'movie', 'he', 'is', 'talking', 'about', '--', 'that', 'is', ',', 'in', 'the', 'original', 'locations', 'and', 'sets', '.', 'The', 'transitions', 'in', 'these', 'sequences', 'sustain', 'such', 'tension', 'and', 'humor', 'that', 'the', 'trick', 'never', 'gets', 'old', '.', 'And', 'Zizek', 'himself', 'is', 'constantly', 'making', 'us', 'laugh', ',', 'either', 'from', 'bizarre', 'little', 'jokes', 'or', 'from', 'his', 'enthusiastic', 'insistence', 'on', ',', 'for', 'example', ',', 'a', 'bold', 'Oedipal', 'interpretation', 'of', '\"', 'The', 'Birds', '.', '\"', 'And', 'this', 'go', '-', 'ahead', '-', 'and', '-', 'laugh', 'attitude', ',', 'on', 'the', 'parts', 'of', 'both', 'Fiennes', 'and', 'Zizek', ',', 'is', 'essential', 'to', 'the', 'gonzo', 'character', 'of', 'the', 'film', '.', 'It', 'is', 'the', 'spoonful', 'of', 'sugar', 'that', 'helps', 'us', 'digest', 'Zizek', \"'s\", 'weird', 'medicine', '.', 'After', 'all', ',', 'do', \"n't\", 'we', 'all', 'have', 'a', 'sense', 'that', ',', 'past', 'a', 'certain', 'point', ',', 'psychology', 'theorists', 'are', 'just', 'pulling', 'our', 'legs', '?'], 'label': 'pos'}\n"]}],"source":["print(vars(train_data.examples[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0CuKM5B1hN3M"},"outputs":[],"source":["import random\n","train_data, valid_data = train_data.split(random_state = random.seed(SEED))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3q4fJrzhN3M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698916956632,"user_tz":-480,"elapsed":3,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"53f956f6-bc6c-4cae-d052-ba086d370ee2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training examples: 1946\n","Number of validation examples: 834\n","Number of testing examples: 2780\n"]}],"source":["print(f'Number of training examples: {len(train_data)}')\n","print(f'Number of validation examples: {len(valid_data)}')\n","print(f'Number of testing examples: {len(test_data)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JMttoY6VhN3N"},"outputs":[],"source":["# MAX_VOCAB_SIZE = 25_000    # In Python 25_000 means 25000\n","MAX_VOCAB_SIZE = 50_000\n","TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n","LABEL.build_vocab(train_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_11cRTEyhN3O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698916957235,"user_tz":-480,"elapsed":10,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"7f281e11-93dc-4773-ca7d-898ee3051b39"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unique tokens in TEXT vocabulary: 31932\n","Unique tokens in LABEL vocabulary: 2\n"]}],"source":["print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n","print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CZeO8Xn1hN3P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698916957235,"user_tz":-480,"elapsed":8,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"f05dcccb-9653-41bf-f05d-023aa99eb0d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["[('the', 22385), (',', 21678), ('.', 18541), ('a', 12623), ('and', 12392), ('of', 11250), ('to', 10503), ('is', 8776), ('in', 7070), ('it', 5694), ('I', 5689), ('that', 5523), ('\"', 5220), (\"'s\", 4948), ('this', 4672), ('-', 4498), ('/><br', 3904), ('was', 3736), ('as', 3541), ('with', 3319)]\n"]}],"source":["print(TEXT.vocab.freqs.most_common(20))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fCcpmZKAhN3P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698916957235,"user_tz":-480,"elapsed":6,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"28fc03b0-34f9-4e09-8109-17f7961e0108"},"outputs":[{"output_type":"stream","name":"stdout","text":["['<unk>', '<pad>', 'the', ',', '.', 'a', 'and', 'of', 'to', 'is']\n"]}],"source":["print(TEXT.vocab.itos[:10])      # Print from the beginning (0th) to the 9th position"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"92P1Qwf_hN3Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698916957235,"user_tz":-480,"elapsed":5,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"47e6fb71-0361-41ed-a8de-b328a21ffa90"},"outputs":[{"output_type":"stream","name":"stdout","text":["defaultdict(None, {'neg': 0, 'pos': 1})\n","0\n"]}],"source":["print(LABEL.vocab.stoi)\n","print(LABEL.vocab.stoi['neg'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bMYQ0yY_hN3Q"},"outputs":[],"source":["BATCH_SIZE = 64\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data, test_data),\n","    batch_size = BATCH_SIZE,\n","    device = device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ouy6f2PxhN3R"},"outputs":[],"source":["import torch.nn as nn\n","\n","class RNN(nn.Module):\n","    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n","\n","        super().__init__()\n","        self.embedding = nn.Embedding(input_dim, embedding_dim)\n","        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, text):\n","        #text = [sent len, batch size]\n","\n","        embedded = self.embedding(text)\n","        #embedded = [sent len, batch size, emb dim]\n","\n","        output, hidden = self.rnn(embedded)\n","        #output = [sent len, batch size, hid dim]\n","        #hidden = [1, batch size, hid dim]\n","        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n","\n","        return self.fc(hidden.squeeze(0))  # Remove the first dim in hidden to return [batch_size, hid_dim]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OAAVZsrEhN3S"},"outputs":[],"source":["INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 256\n","OUTPUT_DIM = 1\n","\n","model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cCz-wugbhN3T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698916957235,"user_tz":-480,"elapsed":3,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"405d8b8d-06fb-4ef9-8093-810052a6a226"},"outputs":[{"output_type":"stream","name":"stdout","text":["The model has 3,285,105 trainable parameters\n"]}],"source":["# Count the number of trainable parameters in our model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FjV4DFiRhN3V"},"outputs":[],"source":["import torch.optim as optim\n","optimizer = optim.SGD(model.parameters(), lr=1e-3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S7YM6fqchN3X"},"outputs":[],"source":["criterion = nn.BCEWithLogitsLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5PvTGEv1hN3Y"},"outputs":[],"source":["model = model.to(device)\n","criterion = criterion.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1gxERCHhhN3Y"},"outputs":[],"source":["def binary_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","    #round predictions to the closest integer\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == y).float() #convert into float for division\n","    acc = correct.sum() / len(correct)\n","    return acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JuLo22nphN3Z"},"outputs":[],"source":["def train(model, iterator, optimizer, criterion, max_samples=2500):\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    model.train()\n","    #for batch in iterator:\n","    for i, batch in enumerate(iterator):\n","        if i > max_samples:                               # Use less samples to reduce training time\n","            break\n","        optimizer.zero_grad()                             # Initialize gradient to 0 for current batch\n","        predictions = model(batch.text).squeeze(1)        # Call RNN.forward() and get return object\n","        loss = criterion(predictions, batch.label)        # Compute binary cross-entropy\n","        acc = binary_accuracy(predictions, batch.label)\n","        loss.backward()                                   # Compute gradient\n","        optimizer.step()                                  # Update the weights by gradient descent\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E96Xue17hN3a"},"outputs":[],"source":["def evaluate(model, iterator, criterion, max_samples=2500):\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    model.eval()\n","    with torch.no_grad():                                    # No gradient will be needed\n","        for batch in iterator:\n","            predictions = model(batch.text).squeeze(1)       # Call RNN.forward()\n","            loss = criterion(predictions, batch.label)       # Compute binary cross entropy loss\n","            acc = binary_accuracy(predictions, batch.label)\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z3vu6477hN3b"},"outputs":[],"source":["import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"An-rmkQ5hN3b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698918240237,"user_tz":-480,"elapsed":1283004,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"201de07d-70a6-4892-ab07-9950f3825738"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 2m 12s\n","\tTrain Loss: 0.694 | Train Acc: 50.10%\n","\t Val. Loss: 0.696 |  Val. Acc: 49.00%\n","Epoch: 02 | Epoch Time: 2m 11s\n","\tTrain Loss: 0.693 | Train Acc: 50.48%\n","\t Val. Loss: 0.696 |  Val. Acc: 48.88%\n","Epoch: 03 | Epoch Time: 2m 15s\n","\tTrain Loss: 0.694 | Train Acc: 50.05%\n","\t Val. Loss: 0.696 |  Val. Acc: 48.88%\n","Epoch: 04 | Epoch Time: 2m 10s\n","\tTrain Loss: 0.693 | Train Acc: 50.50%\n","\t Val. Loss: 0.696 |  Val. Acc: 48.88%\n","Epoch: 05 | Epoch Time: 2m 4s\n","\tTrain Loss: 0.693 | Train Acc: 50.18%\n","\t Val. Loss: 0.696 |  Val. Acc: 48.77%\n","Epoch: 06 | Epoch Time: 2m 5s\n","\tTrain Loss: 0.693 | Train Acc: 49.85%\n","\t Val. Loss: 0.696 |  Val. Acc: 48.88%\n","Epoch: 07 | Epoch Time: 2m 3s\n","\tTrain Loss: 0.694 | Train Acc: 50.35%\n","\t Val. Loss: 0.696 |  Val. Acc: 48.88%\n","Epoch: 08 | Epoch Time: 2m 9s\n","\tTrain Loss: 0.694 | Train Acc: 50.06%\n","\t Val. Loss: 0.696 |  Val. Acc: 48.77%\n","Epoch: 09 | Epoch Time: 2m 4s\n","\tTrain Loss: 0.693 | Train Acc: 50.25%\n","\t Val. Loss: 0.696 |  Val. Acc: 48.66%\n","Epoch: 10 | Epoch Time: 2m 4s\n","\tTrain Loss: 0.694 | Train Acc: 48.91%\n","\t Val. Loss: 0.696 |  Val. Acc: 48.77%\n"]}],"source":["N_EPOCHS = 10\n","best_valid_loss = float('inf')\n","for epoch in range(N_EPOCHS):\n","    start_time = time.time()\n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","    end_time = time.time()\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'tut1-model.pt')\n","\n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LYcc_lsXhN3c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698918244615,"user_tz":-480,"elapsed":4407,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"b7dd2970-5620-4a6f-e829-b7fc443cfc36"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test Loss: 0.693 | Test Acc: 51.84%\n"]}],"source":["model.load_state_dict(torch.load('tut1-model.pt'))\n","test_loss, test_acc = evaluate(model, test_iterator, criterion)\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"]},{"cell_type":"markdown","source":["# **Bidirectional Two-layer LSTM**\n","\n","---\n","\n"],"metadata":{"id":"1d7l2xNjc6cH"}},{"cell_type":"code","metadata":{"id":"K4gU0qNY3ha_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698925081011,"user_tz":-480,"elapsed":15331,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"13f2efd0-88e4-4d93-ce77-c6c93b4201bf"},"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"D5bPMlHJ3pzN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698925081011,"user_tz":-480,"elapsed":3,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"30db8f39-ecb0-4d9e-c08a-e7a501f402c2"},"source":["# Create a working folder and cd to it.\n","!mkdir -p /content/drive/MyDrive/Learning/EIE4122/Lab2\n","%cd /content/drive/MyDrive/Learning/EIE4122/Lab2"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Learning/EIE4122/Lab2\n"]}]},{"cell_type":"code","metadata":{"id":"Dr8j50_DMzBd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698925085670,"user_tz":-480,"elapsed":4660,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"eed9f344-94f0-4850-9723-f925897d77f0"},"source":["# Require gdown to download files from Google Drive\n","!pip install gdown"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.4)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"]}]},{"cell_type":"code","metadata":{"id":"u3_k11-MM8YP"},"source":["# Download the reduced-size IMDB dataset\n","!if [ ! -f data.tgz ]; then gdown https://drive.google.com/uc?id=1EL6YlIEs1IbUDjrZvrZ_9-C0C4gYoryZ; fi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e-t8CNiBNYIg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698925182821,"user_tz":-480,"elapsed":96665,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"2d3dce0f-32b4-4939-ea56-799e44e50c29"},"source":["!echo \"Start decompression. It will take few minutes\"\n","!tar xf data.tgz --exclude=\"._*\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Start decompression. It will take few minutes\n"]}]},{"cell_type":"code","source":["# Install torch 2.0.1 and torchtext 0.5.0\n","!pip install torch==2.0.1 torchtext==0.5.0\n","# you can safely ignore the message \"ERROR: pip's dependency resolver...\" for this lab"],"metadata":{"id":"Q9oQkR0Q_0j3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698925339982,"user_tz":-480,"elapsed":157163,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"3e521128-b01a-4e27-ace2-e69e917a4f99"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch==2.0.1\n","  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchtext==0.5.0\n","  Downloading torchtext-0.5.0-py3-none-any.whl (73 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.2)\n","Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n","  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n","  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n","  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n","  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n","  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n","  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n","  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting triton==2.0.0 (from torch==2.0.1)\n","  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.5.0) (4.66.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.5.0) (2.31.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.5.0) (1.23.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.5.0) (1.16.0)\n","Collecting sentencepiece (from torchtext==0.5.0)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (67.7.2)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.41.2)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.27.7)\n","Collecting lit (from triton==2.0.0->torch==2.0.1)\n","  Downloading lit-17.0.4.tar.gz (153 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.1/153.1 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.5.0) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.5.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.5.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.5.0) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n","Building wheels for collected packages: lit\n","  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for lit: filename=lit-17.0.4-py3-none-any.whl size=93257 sha256=760fe82ce1d23275ccbdd937cdc31a860315c2adfc43b0338157c6f16ebd8908\n","  Stored in directory: /root/.cache/pip/wheels/be/ae/00/696c57d438bfc7c0e89c4c379083ea08b1c2e54d85a5f7cd7c\n","Successfully built lit\n","Installing collected packages: sentencepiece, lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchtext\n","  Attempting uninstall: triton\n","    Found existing installation: triton 2.1.0\n","    Uninstalling triton-2.1.0:\n","      Successfully uninstalled triton-2.1.0\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.1.0+cu118\n","    Uninstalling torch-2.1.0+cu118:\n","      Successfully uninstalled torch-2.1.0+cu118\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.16.0\n","    Uninstalling torchtext-0.16.0:\n","      Successfully uninstalled torchtext-0.16.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n","torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n","torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed lit-17.0.4 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 sentencepiece-0.1.99 torch-2.0.1 torchtext-0.5.0 triton-2.0.0\n"]}]},{"cell_type":"code","metadata":{"id":"5dtj8e002Pd_"},"source":["import torch\n","from torchtext import data\n","\n","SEED = 1234\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","TEXT = data.Field(tokenize = 'spacy',\n","                  tokenizer_language = 'en_core_web_sm',\n","                  include_lengths = True)\n","LABEL = data.LabelField(dtype = torch.float)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0IwuTSs92PeC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698925382660,"user_tz":-480,"elapsed":26147,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"c65e42d0-4f51-4538-aa54-78605f920fac"},"source":["# This step should be quick as PyTorch only needs to know the structure of the data in the\n","# folder \".data/imdb/aclImdb\". It will not actually load the data, which will be done by\n","# the Dataloader object (see code below)\n","print('Loading data from .data/imdb/aclImdb')\n","from torchtext import datasets\n","train_data, test_data = datasets.IMDB.splits(TEXT, LABEL, path='.data/imdb/aclImdb/')\n","print(f'Number of training examples: {len(train_data)}')\n","print(f'Number of testing examples: {len(test_data)}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data from .data/imdb/aclImdb\n","Number of training examples: 2780\n","Number of testing examples: 2780\n"]}]},{"cell_type":"code","metadata":{"id":"xxAmfAch2PeD"},"source":["import random\n","train_data, valid_data = train_data.split(random_state = random.seed(SEED))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UvQjlZfI2PeE"},"source":["MAX_VOCAB_SIZE = 25_000\n","TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n","LABEL.build_vocab(train_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XX-OUZKt2PeE"},"source":["BATCH_SIZE = 64\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data, test_data),\n","    batch_size = BATCH_SIZE,\n","    sort_within_batch = True,\n","    device = device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"63QkigvZ2PeG"},"source":["import torch.nn as nn\n","\n","class RNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n","                 bidirectional, dropout, pad_idx):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n","        self.rnn = nn.LSTM(embedding_dim,\n","                           hidden_dim,\n","                           num_layers=n_layers,\n","                           bidirectional=bidirectional,\n","                           dropout=dropout)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, text, text_lengths):\n","        #text = [sent len, batch size]\n","\n","        embedded = self.dropout(self.embedding(text))\n","        #embedded = [sent len, batch size, emb dim]\n","\n","        #pack sequence, lengths need to be on CPU!\n","        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n","        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n","\n","        #unpack sequence\n","        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n","        #output = [sent len, batch size, hid dim * num directions]\n","        #output over padding tokens are zero tensors\n","\n","        #hidden = [num layers * num directions, batch size, hid dim]\n","        #cell = [num layers * num directions, batch size, hid dim]\n","\n","        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n","        #and apply dropout\n","\n","        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n","        #hidden = [batch size, hid dim * num directions]\n","\n","        return self.fc(hidden)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AB-mNIwo2PeG"},"source":["INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 256\n","OUTPUT_DIM = 1\n","N_LAYERS = 2\n","BIDIRECTIONAL = True\n","DROPOUT = 0.5\n","PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","\n","model = RNN(INPUT_DIM,\n","            EMBEDDING_DIM,\n","            HIDDEN_DIM,\n","            OUTPUT_DIM,\n","            N_LAYERS,\n","            BIDIRECTIONAL,\n","            DROPOUT,\n","            PAD_IDX)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pCpa4Xti2PeH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698925383176,"user_tz":-480,"elapsed":525,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"e498dd68-3f6c-4bcf-a5e2-681145745db4"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The model has 4,810,857 trainable parameters\n"]}]},{"cell_type":"code","metadata":{"id":"QOx4XRPv2PeK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698925383176,"user_tz":-480,"elapsed":3,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"5357a15a-f641-4009-945d-e73a242f386d"},"source":["UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n","model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n","model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n","\n","print(model.embedding.weight.data)\n","print(model.embedding.weight.data.size())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.7289, -0.7336,  1.5624,  ..., -0.5592, -0.4480, -0.6476],\n","        ...,\n","        [ 0.0914,  1.5196,  0.4670,  ...,  0.6393, -0.0332,  0.0185],\n","        [-0.6290,  0.4650, -0.7165,  ..., -1.3171,  2.0381, -2.0497],\n","        [-1.1222, -0.0240, -1.0878,  ..., -0.4948, -0.3874,  0.0339]])\n","torch.Size([25002, 100])\n"]}]},{"cell_type":"code","metadata":{"id":"7WcsJQex2PeL"},"source":["import torch.optim as optim\n","optimizer = optim.Adam(model.parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oveVNuyb2PeL"},"source":["criterion = nn.BCEWithLogitsLoss()\n","model = model.to(device)\n","criterion = criterion.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hcWrFKlt2PeM"},"source":["def binary_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","    #round predictions to the closest integer\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == y).float() #convert into float for division\n","    acc = correct.sum() / len(correct)\n","    return acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"io86dBF-2PeO"},"source":["def train(model, iterator, optimizer, criterion):\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    model.train()\n","\n","    for batch in iterator:\n","        optimizer.zero_grad()\n","        text, text_lengths = batch.text\n","        predictions = model(text, text_lengths).squeeze(1)\n","        loss = criterion(predictions, batch.label)\n","        acc = binary_accuracy(predictions, batch.label)\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0amSnkph2PeR"},"source":["def evaluate(model, iterator, criterion):\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for batch in iterator:\n","            text, text_lengths = batch.text\n","            predictions = model(text, text_lengths).squeeze(1)\n","            loss = criterion(predictions, batch.label)\n","            acc = binary_accuracy(predictions, batch.label)\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0twmWuOW2PeS"},"source":["import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y4mp8CZb2PeS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698925548432,"user_tz":-480,"elapsed":42881,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"65c1d292-32bc-4a63-85bc-dd6e99a4d30a"},"source":["N_EPOCHS = 10\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    start_time = time.time()\n","\n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'tut2-model.pt')\n","\n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.465 | Train Acc: 78.61%\n","\t Val. Loss: 0.635 |  Val. Acc: 67.30%\n","Epoch: 02 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.391 | Train Acc: 82.69%\n","\t Val. Loss: 0.713 |  Val. Acc: 69.20%\n","Epoch: 03 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.406 | Train Acc: 81.43%\n","\t Val. Loss: 0.709 |  Val. Acc: 66.07%\n","Epoch: 04 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.349 | Train Acc: 84.08%\n","\t Val. Loss: 0.654 |  Val. Acc: 71.65%\n","Epoch: 05 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.307 | Train Acc: 86.90%\n","\t Val. Loss: 0.651 |  Val. Acc: 70.87%\n","Epoch: 06 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.249 | Train Acc: 89.80%\n","\t Val. Loss: 0.685 |  Val. Acc: 72.10%\n","Epoch: 07 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.252 | Train Acc: 89.47%\n","\t Val. Loss: 0.935 |  Val. Acc: 72.54%\n","Epoch: 08 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.209 | Train Acc: 91.58%\n","\t Val. Loss: 1.091 |  Val. Acc: 73.33%\n","Epoch: 09 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.219 | Train Acc: 90.40%\n","\t Val. Loss: 0.863 |  Val. Acc: 71.54%\n","Epoch: 10 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.195 | Train Acc: 91.89%\n","\t Val. Loss: 0.887 |  Val. Acc: 72.66%\n"]}]},{"cell_type":"code","metadata":{"id":"ZD5spO-l2PeT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698925549944,"user_tz":-480,"elapsed":1521,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"bf7d8c5a-7fa9-491b-81cf-289424465cd7"},"source":["model.load_state_dict(torch.load('tut2-model.pt'))\n","test_loss, test_acc = evaluate(model, test_iterator, criterion)\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Loss: 0.679 | Test Acc: 64.45%\n"]}]},{"cell_type":"code","metadata":{"id":"tpNlt01Z2PeU"},"source":["import spacy\n","nlp = spacy.load('en_core_web_sm')\n","\n","def predict_sentiment(model, sentence):\n","    model.eval()\n","    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n","    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n","    length = [len(indexed)]\n","    tensor = torch.LongTensor(indexed).to(device)\n","    tensor = tensor.unsqueeze(1)\n","    length_tensor = torch.LongTensor(length)\n","    prediction = torch.sigmoid(model(tensor, length_tensor))\n","    return prediction.item()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NjYvi35L2PeV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698925628991,"user_tz":-480,"elapsed":257,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"998b8c9a-5c66-4d27-e0b1-30bbf5dd3a5b"},"source":["predict_sentiment(model, \"The movie is awesome!.\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7324489951133728"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"GjPpyUD12PeV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698925644356,"user_tz":-480,"elapsed":3,"user":{"displayName":"Jiadong HAO","userId":"11340680302583385399"}},"outputId":"8d4afc70-dac5-4bcc-d2f3-b7f93bdf83ed"},"source":["predict_sentiment(model, \"This film is terrible!.\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.34864652156829834"]},"metadata":{},"execution_count":32}]}]}